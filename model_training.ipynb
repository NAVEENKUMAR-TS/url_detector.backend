# ==========================================
# URL Safety Detector - Colab-ready
# 0 = malicious, 1 = safe
# ==========================================
# 1. SETUP & IMPORTS
# ==========================================
!pip install -q tensorflow==2.14.1  # pin to known TF for Colab compatibility (optional)
!pip install -q scikit-learn requests

import os
import numpy as np
import pandas as pd
import pickle
import requests
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix
from google.colab import drive, files

# Mount Drive (optional; remove if not using Drive)
print("Mounting Drive (if needed)...")
try:
    drive.mount('/content/drive')
except Exception as e:
    print("Drive mount skipped or failed:", e)

# ==========================================
# 2. CONFIGURATION
# ==========================================
MAX_LEN = 150       # Max tokens (characters) per URL
VOCAB_SIZE = 2000   # tokenizer max words (not strictly enforced by Keras Tokenizer)
EMBEDDING_DIM = 64
BATCH_SIZE = 64
EPOCHS = 12
RANDOM_STATE = 42

# ==========================================
# 3. DATA LOADING & CLEANING
# ==========================================
print("\nLoading Dataset...")
# Edit this path if your dataset is elsewhere (Drive or uploaded)
file_path = '/content/drive/MyDrive/PhiUSIIL_Phishing_URL_Dataset.csv'  # or '/content/dataset.csv' if you uploaded directly

if not os.path.exists(file_path):
    # try to let user upload if path doesn't exist
    from google.colab import files as colab_files
    print("Dataset not found at path. Please upload dataset.csv now (it must contain columns 'URL' and 'label').")
    uploaded = colab_files.upload()
    if len(uploaded) == 0:
        raise SystemExit("No dataset provided. Re-run and upload a CSV with 'URL' and 'label' columns.")
    file_path = list(uploaded.keys())[0]

df = pd.read_csv(file_path)
print("Raw columns:", df.columns.tolist())

# Normalize column names (case-insensitive)
col_map = {c: c for c in df.columns}
lower_map = {c.lower(): c for c in df.columns}
if 'url' in lower_map:
    col_map['URL'] = lower_map['url']
if 'label' in lower_map:
    col_map['label'] = lower_map['label']
df = df.rename(columns=col_map)

# Keep only URL and label
if 'URL' not in df.columns or 'label' not in df.columns:
    raise SystemExit("Dataset must contain 'URL' and 'label' columns (case-insensitive).")

df = df[['URL', 'label']].copy()
df.dropna(subset=['URL', 'label'], inplace=True)
df = df[df['URL'].astype(str).str.strip() != ""]
# ensure label is integer
df['label'] = df['label'].astype(int)

# IMPORTANT: User-specified mapping: 0 = malicious, 1 = safe
# If your dataset uses a different mapping, convert it here.

print("\nOriginal label distribution:")
print(df['label'].value_counts())

# ==========================================
# 4. BALANCING (oversample minority)
# ==========================================
# Determine majority/minority
counts = df['label'].value_counts()
if len(counts) == 1:
    print("Only one class present in dataset; training won't be meaningful.")
else:
    maj_class = counts.idxmax()
    min_class = counts.idxmin()
    n_samples = counts.max()
    print(f"\nBalancing: oversample class {min_class} to {n_samples} samples (majority={maj_class}).")

    df_major = df[df['label'] == maj_class]
    df_min = df[df['label'] == min_class]

    if len(df_min) < n_samples:
        df_min_upsampled = resample(df_min,
                                    replace=True,
                                    n_samples=n_samples,
                                    random_state=RANDOM_STATE)
    else:
        df_min_upsampled = df_min

    df = pd.concat([df_major, df_min_upsampled]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)

print("\nPost-balance distribution:")
print(df['label'].value_counts())

# ==========================================
# 5. TOKENIZATION (character-level)
# ==========================================
print("\nTokenizing URLs (character-level)...")
tokenizer = Tokenizer(char_level=True, lower=True, filters=None)  # keep punctuation
tokenizer.fit_on_texts(df['URL'].astype(str))
sequences = tokenizer.texts_to_sequences(df['URL'].astype(str))
X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')

# Prepare labels: we want categorical (2 classes) where index 0 corresponds to label 0 (malicious)
y = tf.keras.utils.to_categorical(df['label'].values, num_classes=2)

# Save tokenizer for later
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
print("Tokenizer saved as 'tokenizer.pickle'.")

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=df['label'].values)
print("Train/Test shapes:", X_train.shape, X_test.shape)

# ==========================================
# 6. MODEL ARCHITECTURE (CNN + BiLSTM)
# ==========================================
print("\nBuilding model...")
vocab_size_actual = len(tokenizer.word_index) + 1
model = Sequential([
    Embedding(input_dim=vocab_size_actual, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    # optional pooling to reduce sequence length before LSTM
    # GlobalMaxPooling1D(),  # removed because we'll use LSTM afterward
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(2, activation='softmax')  # 2 output classes: index 0 -> malicious, index 1 -> safe
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ==========================================
# 7. TRAINING (with EarlyStopping)
# ==========================================
print("\nTraining...")
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[early_stop]
)

# ==========================================
# 8. EVALUATION & EXPORT
# ==========================================
print("\nEvaluation on test set:")
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test loss: {loss:.4f}, Test accuracy: {acc*100:.2f}%")

y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

print("\nClassification report (0=malicious, 1=safe):")
print(classification_report(y_true, y_pred, digits=4))
print("Confusion matrix:\n", confusion_matrix(y_true, y_pred))

# Save model and tokenizer
model_save_path = 'url_deep_model.keras'
model.save(model_save_path)
print(f"Model saved as '{model_save_path}'.")
files.download(model_save_path)
files.download('tokenizer.pickle')

# ==========================================
# 9. MANUAL TESTING (interactive)
# ==========================================
print("\n--- Manual Test ---")
test_urls = [
    "https://www.google.com",
    "http://evil-phishing-site.com/login.php",
    "http://secure-login.google.com.verify-account.com?ref=secure"
]

# Prepare sequences
test_seq = tokenizer.texts_to_sequences(test_urls)
test_padded = pad_sequences(test_seq, maxlen=MAX_LEN, padding='post', truncating='post')

preds = model.predict(test_padded)
classes = np.argmax(preds, axis=1)
labels_map = {0: "Malicious", 1: "Safe"}

print(f"{'URL':<60} | {'Prediction':<12} | {'Confidence(%)':<12}")
print("-" * 90)
for i, url in enumerate(test_urls):
    print(f"{(url[:57] + '...') if len(url)>60 else url:<60} | {labels_map[classes[i]]:<12} | {np.max(preds[i])*100:6.2f}%")

# Also allow user to input a URL for immediate classification
try:
    user_url = input("\nEnter a URL to classify (or press Enter to skip): ").strip()
except Exception:
    user_url = ""

if user_url:
    seq = tokenizer.texts_to_sequences([user_url])
    pad = pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')
    p = model.predict(pad)[0]
    cl = int(np.argmax(p))
    conf = float(np.max(p))
    print(f"\nURL: {user_url}")
    print(f"Prediction: {labels_map[cl]} ({cl})  Confidence: {conf*100:.2f}%")

    # ==========================================
    # 10. SEND TO GEMINI API FOR EXPLANATION
    # ==========================================
    # Replace these with your actual Gemini endpoint and API key
    GEMINI_API_KEY = 'AIzaSyA6KAatJgK5KrfxyooERK4IEp0FE41otHM'
    GEMINI_ENDPOINT = 'https://api.openai.com/v1/responses'  # replace if your environment uses a different endpoint

    if GEMINI_API_KEY.startswith('REPLACE'):
        print("\nSkipping Gemini call because GEMINI_API_KEY is not set. Replace GEMINI_API_KEY to enable.")
    else:
        # Build a helpful prompt: include URL, model prediction, confidence and a short list of token counts
        token_chars = {ch: tokenizer.word_index.get(ch, 0) for ch in sorted(set(user_url))}
        prompt_text = (
            f"I have the following URL: {user_url}\n\n"
            f"Model prediction: {labels_map[cl]} (label={cl})\n"
            f"Confidence for predicted class: {conf:.4f}\n\n"
            f"Please explain, in concise points, why this URL is likely malicious or safe. "
            "Mention any visible tokens, patterns, suspicious substrings, or structural cues you see in the URL "
            "that support the classification. Keep the explanation short (3-6 bullet points).\n"
        )

        payload = {
            "model": "gpt-4o-mini",  # change to your preferred Gemini model name
            "input": prompt_text
        }
        headers = {
            "Authorization": f"Bearer {GEMINI_API_KEY}",
            "Content-Type": "application/json"
        }

        print("\nSending request to Gemini for an explanation...")
        try:
            resp = requests.post(GEMINI_ENDPOINT, headers=headers, json=payload, timeout=30)
            if resp.status_code == 200:
                resp_json = resp.json()
                # Try to extract a human-readable text from common response structures
                explanation = None
                # common OpenAI-style fields
                if isinstance(resp_json, dict):
                    # try `output` -> list -> content -> text
                    if 'output' in resp_json and isinstance(resp_json['output'], list) and len(resp_json['output'])>0:
                        # each item may have 'content' list of dicts with 'text'
                        out = resp_json['output'][0]
                        if isinstance(out, dict) and 'content' in out and isinstance(out['content'], list):
                            explanation = "\n".join([c.get('text','') for c in out['content'] if isinstance(c, dict) and c.get('text')])
                    # try older 'choices' structure
                    if not explanation and 'choices' in resp_json and len(resp_json['choices'])>0:
                        ch = resp_json['choices'][0]
                        if 'message' in ch and 'content' in ch['message']:
                            content = ch['message']['content']
                            if isinstance(content, list):
                                explanation = "\n".join([c.get('text','') for c in content if isinstance(c, dict) and c.get('text')])
                            else:
                                explanation = str(content)
                if not explanation:
                    # fallback: print entire JSON (truncated)
                    print("Could not extract nicely formatted text. Raw JSON (truncated):")
                    print(resp_json)
                else:
                    print("\nGemini explanation:\n")
                    print(explanation)
            else:
                print("Gemini API returned status", resp.status_code)
                print(resp.text)
        except Exception as e:
            print("Failed to contact Gemini endpoint:", e)

print("\nNotebook run finished. Saved files: 'url_deep_model.keras' and 'tokenizer.pickle' (downloaded).")


import google.genai as genai

# Your Gemini API Key
client = genai.Client(api_key="AIzaSyA1uB-qzLUWbIUAlnh8jgmdxfW2o2ZPdnk")

def explain_with_gemini(url, label, confidence):
    prompt = f"""
    URL: {url}
    Model Prediction: {label}
    Confidence: {confidence}

    Explain briefly why this URL is likely safe or malicious.
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",   # Use the model your key supports
            contents=prompt
        )
        return response.text
    except Exception as e:
        return f"Gemini error: {str(e)}"


# ==========================================
# MANUAL TEST + GEMINI EXPLANATION
# ==========================================

print("\n--- Manual Test ---")
test_urls = [
    "https://www.google.com",
    "http://evil-phishing-site.com/login.php",
    "http://secure-login.google.com.verify-account.com?ref=secure"
]

# Prepare sequences
test_seq = tokenizer.texts_to_sequences(test_urls)
test_padded = pad_sequences(test_seq, maxlen=MAX_LEN, padding='post', truncating='post')

preds = model.predict(test_padded)
classes = np.argmax(preds, axis=1)
labels_map = {0: "Malicious", 1: "Safe"}

print(f"{'URL':<60} | {'Prediction':<12} | {'Confidence(%)':<12}")
print("-" * 90)

# Loop through all URLs
for i, url in enumerate(test_urls):
    pred_label = labels_map[classes[i]]
    confidence = float(np.max(preds[i]))

    print(f"{(url[:57] + '...') if len(url)>60 else url:<60} | {pred_label:<12} | {confidence*100:6.2f}%")

    # Call Gemini for explanation
    explanation = explain_with_gemini(url, pred_label, confidence)

    print("\n--- Gemini Explanation ---")
    print(explanation)
    print("-" * 90)


----------------------------------


#!pip install tensorflow google-genai

import tensorflow as tf
import pickle
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

# -----------------------------
# Load trained model + tokenizer
# -----------------------------
MODEL_PATH = "/content/url_deep_model.keras"      # change if needed
TOKENIZER_PATH = "/content/tokenizer.pickle"      # change if needed

model = tf.keras.models.load_model(MODEL_PATH)
print("Model loaded.")

with open(TOKENIZER_PATH, "rb") as handle:
    tokenizer = pickle.load(handle)
print("Tokenizer loaded.")

# -----------------------------
# Prediction function
# -----------------------------
MAX_LEN = 150  # must match your training configuration
labels_map = {0: "Malicious", 1: "Safe"}

def predict_url(url):
    seq = tokenizer.texts_to_sequences([url])
    padded = pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')
    pred = model.predict(padded)[0]
    cls = int(np.argmax(pred))
    conf = float(np.max(pred))
    return labels_map[cls], cls, conf


import google.genai as genai

# Replace with your real key
client = genai.Client(api_key="AIzaSyA1uB-qzLUWbIUAlnh8jgmdxfW2o2ZPdnk")

def gemini_explain(url, label, confidence):
    prompt = f"""
    URL: {url}
    Prediction: {label}
    Confidence: {confidence:.4f}

    Explain in one line why the url is likely as the prediction and confidence suggest. also if the given url is to be adversarial then metion is type and insights about that.
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        return response.text
    except Exception as e:
        return f"[Gemini Error] {str(e)}"


test_urls = [
    "https://www.gooogle.example"
]



print(f"{'URL':<60} | {'Prediction':<10} | {'Confidence':<12}")
print("-" * 90)

for url in test_urls:
    label, cls, conf = predict_url(url)
    
    print(f"{(url[:57] + '...') if len(url)>60 else url:<60} | {label:<10} | {conf*100:6.2f}%")

    # Gemini explanation
    explanation = gemini_explain(url, label, conf)
    print("\n--- Gemini Explanation ---")
    print(explanation)
    print("-" * 90)


import random
import string
import codecs
from urllib.parse import urlparse, urlunparse, quote_plus, urlencode

# ---------------------------
# Advanced Adversarial URL Generator (safe by default)
# ---------------------------
HOMOGLYPH_MAP = {
    'a': ['à','á','â','ä','ɑ','@'],
    'b': ['Ƅ','ƅ','6'],
    'c': ['ç','ć','č'],
    'd': ['đ'],
    'e': ['è','é','ê','ë','3'],
    'g': ['9'],
    'i': ['1','ì','í','î','ï','ı','|','!'],
    'l': ['1','ı','|'],
    'o': ['0','ò','ó','ô','ö','ø','ō'],
    's': ['5','$'],
    't': ['7','ţ'],
    'u': ['ù','ú','û','ü'],
    'y': ['ÿ'],
    'n': ['ñ'],
    'm': ['nn'],
    'h': ['ḥ']
}

RESERVED_TEST_TLD = ".example"   # safe by default - non-resolving testing suffix

def _rand_insert_token():
    tokens = ['secure', 'login', 'verify', 'account', 'update', 'auth', 'confirm', 'support', 'billing']
    return random.choice(tokens) + str(random.randint(1,9999))

def _replace_tld_safe(domain):
    # Replace TLD with reserved test TLD so variants are non-resolving
    parts = domain.split('.')
    # keep subdomain parts if present, set top-level to .example
    if len(parts) >= 2:
        parts[-1] = ''  # drop original tld
        new = '.'.join([p for p in parts if p]) + RESERVED_TEST_TLD
    else:
        new = domain + RESERVED_TEST_TLD
    return new

def _homoglyph_substitute(name, max_subs=2):
    chars = list(name)
    idxs = [i for i,c in enumerate(chars) if c.lower() in HOMOGLYPH_MAP]
    random.shuffle(idxs)
    subs = 0
    for i in idxs:
        if subs >= max_subs:
            break
        cmap = HOMOGLYPH_MAP.get(chars[i].lower())
        if cmap:
            sub = random.choice(cmap)
            chars[i] = sub
            subs += 1
    return ''.join(chars)

def _bitsquat_like(name, flips=1):
    # not actual bitsquatting on DNS bytes, approximate by flipping adjacent characters or char-bitwise flip
    res = list(name)
    for _ in range(flips):
        i = random.randrange(len(res))
        c = res[i]
        # simple variations: replace char with nearby keyboard/number or shift ascii
        if c.isalpha():
            res[i] = chr(((ord(c.lower()) - 97 + random.choice([-1,1,2])) % 26) + 97)
        elif c.isdigit():
            res[i] = str((int(c) + random.choice([1,9])) % 10)
        else:
            res[i] = c
    return ''.join(res)

def _transpose_chars(name):
    if len(name) < 2:
        return name
    i = random.randrange(len(name)-1)
    l = list(name)
    l[i], l[i+1] = l[i+1], l[i]
    return ''.join(l)

def _insert_dots_between(name, max_dots=2):
    # insert dots in subdomain segments to create confusing subdomain levels
    chars = list(name)
    for _ in range(random.randint(1, max_dots)):
        i = random.randrange(1, len(chars))
        chars.insert(i, '.')
    return ''.join(chars)

def _punycode_simulate(name):
    # Instead of generating real IDN/punycode we simulate by prefixing with 'xn--' and altered name
    altered = name[:6] + ('-' if len(name)>6 else '') + 'xn'
    return 'xn--' + altered

def _percent_encode(path, ratio=0.4):
    # percent-encode random subset of path characters
    out = []
    for ch in path:
        if random.random() < ratio and ch not in '/':
            out.append('%' + format(ord(ch), 'x'))
        else:
            out.append(ch)
    return ''.join(out)

def generate_advanced_adversarial_urls(
    url,
    n=20,
    seed=None,
    safe_mode=True,
    strategies=None
):
    """
    Generate advanced adversarial URL variants for a given URL.
    - url: input URL string
    - n: number of variants desired (function returns <= n unique variants)
    - seed: optional int for reproducibility
    - safe_mode: True => replace TLDs with a non-resolving suffix (.example) to avoid creating live malicious URLs
    - strategies: optional list of strategy names to restrict generation
    Returns: list of unique URL strings
    """

    if seed is not None:
        random.seed(seed)

    parsed = urlparse(url if "://" in url else "http://" + url)
    scheme = parsed.scheme or 'http'
    host = (parsed.hostname or '').strip()
    path = parsed.path or ''
    query = parsed.query or ''
    fragment = parsed.fragment or ''

    # parse domain components
    host_parts = host.split('.') if host else []
    main = host_parts[-2] if len(host_parts) >= 2 else (host_parts[0] if host_parts else '')
    tld = host_parts[-1] if len(host_parts) >= 2 else ''
    subdomain = '.'.join(host_parts[:-2]) if len(host_parts) > 2 else ''

    # default strategies
    ALL_STRATS = [
        "typo_swap", "homoglyph", "bitsquat_like", "transpose",
        "subdomain_injection", "dot_insertion", "tld_swap", "path_obfuscation",
        "param_injection", "percent_encode", "punycode_like", "fake_ip_embed",
        "trusted_prepended", "long_path", "redirect_chain", "token_append",
        "service_impersonation", "double_subdomain"
    ]
    if strategies is None:
        strategies = ALL_STRATS
    else:
        strategies = [s for s in strategies if s in ALL_STRATS]

    variants = set()

    def make_host(new_host):
        if safe_mode:
            # convert any TLD to .example to avoid real targets
            return _replace_tld_safe(new_host)
        else:
            return new_host

    attempts = 0
    max_attempts = n * 10

    while len(variants) < n and attempts < max_attempts:
        attempts += 1
        strat = random.choice(strategies)

        # default base host (maybe with subdomain)
        base_host = host
        if strat == "typo_swap":
            # simple character deletion/insertion/replacement
            if main:
                mode = random.choice(['del','dup','replace'])
                if mode == 'del' and len(main) > 3:
                    new_main = main[:random.randint(0,len(main)-1)] + main[random.randint(0,len(main)-1)+1:]
                elif mode == 'dup' and len(main) > 1:
                    i = random.randrange(len(main))
                    new_main = main[:i] + main[i] + main[i:]
                else:
                    i = random.randrange(len(main))
                    new_main = main[:i] + random.choice(string.ascii_lowercase) + main[i+1:]
                new_host = new_main + ('.' + tld if tld else '')
                new_host = (subdomain + '.' + new_host) if subdomain else new_host
                base_host = make_host(new_host)

        elif strat == "homoglyph":
            if main:
                new_main = _homoglyph_substitute(main, max_subs=3)
                new_host = new_main + ('.' + tld if tld else '')
                new_host = (subdomain + '.' + new_host) if subdomain else new_host
                base_host = make_host(new_host)

        elif strat == "bitsquat_like":
            if main:
                new_main = _bitsquat_like(main, flips=random.randint(1,2))
                new_host = new_main + ('.' + tld if tld else '')
                new_host = (subdomain + '.' + new_host) if subdomain else new_host
                base_host = make_host(new_host)

        elif strat == "transpose":
            if main:
                new_main = _transpose_chars(main)
                new_host = new_main + ('.' + tld if tld else '')
                new_host = (subdomain + '.' + new_host) if subdomain else new_host
                base_host = make_host(new_host)

        elif strat == "subdomain_injection":
            token = _rand_insert_token()
            new_host = token + '.' + (host or main + ('.'+tld if tld else ''))
            base_host = make_host(new_host)

        elif strat == "double_subdomain":
            # create deep nested subdomain using main repeated with dash or keywords
            new_host = f"{main}-{_rand_insert_token()}.{main}.{tld}" if main and tld else host
            base_host = make_host(new_host)

        elif strat == "dot_insertion":
            if main:
                new_main = _insert_dots_between(main, max_dots=2)
                new_host = new_main + ('.' + tld if tld else '')
                new_host = (subdomain + '.' + new_host) if subdomain else new_host
                base_host = make_host(new_host)

        elif strat == "tld_swap":
            # swap TLD to suspicious ones but in safe_mode replaced with example
            bad_tlds = ['xyz','top','click','online','site','info','biz']
            new_tld = random.choice(bad_tlds)
            new_host = '.'.join(host_parts[:-1] + [new_tld]) if host_parts else host + '.' + new_tld
            base_host = make_host(new_host)

        elif strat == "path_obfuscation":
            new_path = path.rstrip('/') + '/' + '/'.join([_rand_insert_token(), 'login'])
            base_host = make_host(host)
            constructed = urlunparse((scheme, base_host, new_path, '', '', ''))
            variants.add(constructed)
            continue

        elif strat == "long_path":
            long_path = '/'.join(['login'] + [_rand_insert_token() for _ in range(random.randint(3,6))])
            base_host = make_host(host)
            constructed = urlunparse((scheme, base_host, '/' + long_path, '', '', ''))
            variants.add(constructed)
            continue

        elif strat == "param_injection":
            params = {'utm_source': 'email', 'verify': 'true', 'session': str(random.randint(1000,99999))}
            base_host = make_host(host)
            constructed = urlunparse((scheme, base_host, path if path else '/', '', urlencode(params), ''))
            variants.add(constructed)
            continue

        elif strat == "percent_encode":
            new_path = _percent_encode(path if path else '/login', ratio=0.5)
            base_host = make_host(host)
            constructed = urlunparse((scheme, base_host, new_path, '', query, ''))
            variants.add(constructed)
            continue

        elif strat == "punycode_like":
            if main:
                pn = _punycode_simulate(main)
                new_host = pn + ('.' + tld if tld else '')
                base_host = make_host(new_host)

        elif strat == "fake_ip_embed":
            # embed fake ip as prefix: 192.0.2.123.main.com (use TEST domain conversion)
            fake_ip = f"192.0.2.{random.randint(1,254)}"
            new_host = f"{fake_ip}.{host}" if host else fake_ip
            base_host = make_host(new_host)

        elif strat == "trusted_prepended":
            # prepend trusted service (e.g., accounts.google) to domain
            trusted = random.choice(['accounts', 'secure', 'login', 'support'])
            new_host = f"{trusted}.{host}"
            base_host = make_host(new_host)

        elif strat == "redirect_chain":
            base_host = make_host(host)
            # create redirect param pointing to a fake target
            redirect_to = f"https://{main}.malicious-example{RESERVED_TEST_TLD}/"
            params = {'next': redirect_to, 'token': str(random.randint(1000,9999))}
            constructed = urlunparse((scheme, base_host, path if path else '/', '', urlencode(params), ''))
            variants.add(constructed)
            continue

        elif strat == "token_append":
            token = _rand_insert_token()
            constructed = urlunparse((scheme, make_host(host), path + '/' + token, '', '', ''))
            variants.add(constructed)
            continue

        elif strat == "service_impersonation":
            service = random.choice(['google-docs','microsoft-auth','appleid','paypal-secure'])
            new_host = f"{service}.{main}.{tld}" if main and tld else f"{service}.{host}"
            base_host = make_host(new_host)

        else:
            base_host = make_host(host)

        # After computing base_host, construct paths/queries sometimes
        path_choice = path
        if random.random() < 0.5:
            # append suspicious path suffix
            path_choice = (path.rstrip('/') if path else '') + '/' + random.choice(['login','signin','auth','verify','secure'])
        if random.random() < 0.4:
            # add query param
            params = {'id': str(random.randint(1000,9999))}
            # sometimes include redirect param
            if random.random() < 0.3:
                params['redirect'] = 'https://example.com/'
            constructed = urlunparse((scheme, base_host, path_choice, '', urlencode(params), fragment))
        else:
            constructed = urlunparse((scheme, base_host, path_choice, '', query, fragment))

        # final touch: optionally percent-encode small fraction
        if random.random() < 0.2 and '?' not in constructed:
            constructed = constructed.replace('/', '/%2F', 1)  # minor obfuscation

        variants.add(constructed)

    # guarantee deterministic order when seed set
    variants = list(variants)
    if seed is not None:
        variants.sort()
    # return up to n
    return variants[:n]

# ---------------------------
# Example usage
# ---------------------------
if __name__ == "__main__":
    src = "https://www.google.com"
    advs = generate_advanced_adversarial_urls(src, n=20, seed=42, safe_mode=True)
    for u in advs:
        print(u)
